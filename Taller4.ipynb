{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a91192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Obtén el ID del archivo\n",
    "# De tu URL: https://drive.google.com/file/d/1sNbDDK1wfmk7X0uyEKA89WHtmKoJBSjc/view\n",
    "# El ID es: 1sNbDDK1wfmk7X0uyEKA89WHtmKoJBSjc\n",
    "FILE_ID = '1sNbDDK1wfmk7X0uyEKA89WHtmKoJBSjc'\n",
    "\n",
    "# 2. Crea la URL de exportación directa\n",
    "DOWNLOAD_URL = f'https://drive.google.com/uc?export=download&id={FILE_ID}'\n",
    "\n",
    "df = pd.read_csv(DOWNLOAD_URL, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9047e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483a4db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff92993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) Identifique observaciones que puedan considerarse problemáticas (datos atípicos,puntos de balanceo e influyentes) y analice si debe eliminarlas de su conjunto de\n",
    "#datos o no, justifique. Repita la construcción del modelo de regresión si eliminó observaciones.\n",
    "\n",
    "# Modelo inicial\n",
    "X = df[[\"X1\",\"X2\",\"X3\",\"X4\"]]\n",
    "X = sm.add_constant(X)\n",
    "y = df[\"Y\"]\n",
    "\n",
    "modelo = sm.OLS(y, X).fit()\n",
    "\n",
    "modelo.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388ce2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "influence = modelo.get_influence()\n",
    "#----------------------------------------------------------------------------------\n",
    "# Distancia de Cook\n",
    "cooks_d = pd.Series(influence.cooks_distance[0], index=df.index)\n",
    "\n",
    "umbral_cook = 4 / len(df)\n",
    "influence_points = cooks_d[cooks_d > umbral_cook]  # ya mantiene los índices del df\n",
    "indices_cook = list(influence_points.index)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "print(f\"Hay {len(influence_points)} puntos influyentes.\")\n",
    "print(\"Índices influyentes en el DataFrame mediante Cook:\", list(indices_cook))\n",
    "print(\"\\nValores de Cook correspondientes:\\n\", influence_points)\n",
    "\n",
    "\n",
    "# Visualizar posibles influyentes\n",
    "plt.stem(np.arange(len(cooks_d)), cooks_d)\n",
    "plt.axhline(4/len(df), color=\"r\", linestyle=\"--\")\n",
    "plt.show()\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Gráfico de influencia\n",
    "sm.graphics.influence_plot(modelo, criterion=\"cooks\")\n",
    "modelo.get_influence().resid_studentized_internal\n",
    "plt.show()\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "#----------------------------------------------------------------------------------\n",
    "# Puntos de balanceo\n",
    "influential_points = modelo.get_influence().summary_frame()\n",
    "print(influential_points.head())\n",
    "\n",
    "n = len(df)\n",
    "k = 4\n",
    "print(n, k)\n",
    "influential_points = modelo.get_influence().summary_frame()\n",
    "influential_points['leverage'] = modelo.get_influence().hat_matrix_diag\n",
    "influential_points['studentized_residuals'] = modelo.get_influence().resid_studentized_internal\n",
    "threshold = 2 * (k + 1) / n\n",
    "outliers = influential_points[influential_points['leverage'] > threshold]\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "print(f\" Los puntos de balanceo son {len(outliers)}\") # al parecer ninguno supera\n",
    "\n",
    "##DDFITS\n",
    "influential_points['ddfits'] = modelo.get_influence().dffits[0]\n",
    "ddfits_threshold = 2 * np.sqrt((k + 1) / n)\n",
    "outliers_ddfits = influential_points[abs(influential_points['ddfits']) > ddfits_threshold]\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "print(f\" Los puntos de balanceo con DFFITS son {len(outliers_ddfits)} equivalentes a {outliers_ddfits}\") #indices 15 y 22\n",
    "\n",
    "\n",
    "# índices de los puntos con DFFITS fuera del umbral:\n",
    "outlier_indices_ddfits = influential_points.index[abs(influential_points['ddfits']) > ddfits_threshold]\n",
    "indices_ddfits = list(outliers_ddfits.index)\n",
    "\n",
    "#Indices de distancias de cook y dffits\n",
    "indices_totales = sorted(set(indices_cook) | set(indices_ddfits))\n",
    "\n",
    "df_clean = df.drop(index=indices_totales) # Se quitan los outliders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9208c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# volvemos a construir el modelo sin outliders para comparar\n",
    "X = df_clean[[\"X1\",\"X2\",\"X3\",\"X4\"]]\n",
    "X = sm.add_constant(X)\n",
    "y = df_clean[\"Y\"]\n",
    "\n",
    "modelo = sm.OLS(y, X).fit()\n",
    "\n",
    "modelo.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e91907",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2) Realice la prueba de significancia del modelo, interprete.\n",
    "\n",
    "\n",
    "modelo = smf.ols('Y ~ X1 + X2 + X3 + X4', data=df_clean).fit()\n",
    "Anova_Table = anova_lm(modelo, typ=2)\n",
    "F_value= Anova_Table['F'].iloc[0]\n",
    "p_value= Anova_Table['PR(>F)'].iloc[0]\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "print(Anova_Table)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Definimos las hipotesis\n",
    "# Ho: β1 = β2 = ... = βk = 0\n",
    "# Ha: Al menos un βi ≠ 0 para i = 1, 2, ..., k\n",
    "\n",
    "if p_value < 0.05:\n",
    "  print(\"Rechazamos la hipótesis nula, y por tanto el modelo tiene significancia.\")\n",
    "else:\n",
    "    print(\"No rechazamos la hipótesis nula, y por tanto el modelo no tiene significancia.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6db5aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Obtener el coeficiente de determinación y el coeficiente de determinación ajustado. Interprete.\n",
    "\n",
    "\n",
    "Rsquared_Modelo_Inicial =\t0.963\n",
    "Rsquared_adj_Modelo_Inicial =\t0.955\n",
    "Rsquared_Modelo_Nuevo =\t0.968\n",
    "Rsquared_adj_Modelo_Nuevo =\t0.961\n",
    "\n",
    "if Rsquared_Modelo_Nuevo > Rsquared_Modelo_Inicial and Rsquared_adj_Modelo_Nuevo > Rsquared_adj_Modelo_Inicial:\n",
    "  print(\"El nuevo modelo tiene mejor ajuste.\")\n",
    "else: print(\"El nuevo modelo no tiene mejor ajuste.\")\n",
    "\n",
    "\n",
    "#Ahora, para definir si continuamos con (o sin) los outliers identificados, procedemos a comparar el el R y el R-ajustado del modelo con y sin los outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46057b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#d) Analice si hay problemas de multicolinealidad.\n",
    "\n",
    "corr_matrix = df.corr()\n",
    "print(corr_matrix)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', square=True)\n",
    "plt.title(\"Matriz de correlación\")\n",
    "plt.show()\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Identificar variables altamente correlacionadas\n",
    "threshold = 0.5\n",
    "high_corr = np.where((corr_matrix.abs() > threshold) & (corr_matrix.abs() < 1))\n",
    "high_corr_pairs = [(corr_matrix.index[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]) for i, j in zip(*high_corr) if i < j]\n",
    "print(\"Variables altamente correlacionadas (|corr| > 0.8):\")\n",
    "for var1, var2, corr_value in high_corr_pairs:\n",
    "    print(f\"{var1} y {var2}: {corr_value:.2f}\")\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Evaluamos multocolineidad aplicando el factor de inflación de la varianza (VIF)\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Variable\"] = X.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i)\n",
    "                   for i in range(X.shape[1])]\n",
    "print(vif_data)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "print(f\" Hay {len(vif_data[vif_data['VIF'] > 10])} variables con multicolinealidad severa equivalentes a {vif_data[vif_data['VIF'] > 10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff39234a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#e) Realice una selección de variables por el método que prefiera, tome decisiones, explique.\n",
    "\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan, normal_ad\n",
    "from scipy import stats\n",
    "\n",
    "residuos = modelo.resid\n",
    "ajustados = modelo.fittedvalues\n",
    "\n",
    "# --- 3.1 Linealidad y homocedasticidad ---\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.scatterplot(x=ajustados, y=residuos)\n",
    "plt.axhline(0, color=\"red\", linestyle=\"--\")\n",
    "plt.xlabel(\"Valores ajustados\")\n",
    "plt.ylabel(\"Residuos\")\n",
    "plt.title(\"Residuos vs valores ajustados\")\n",
    "plt.show()\n",
    "\n",
    "# Test de Breusch-Pagan para heterocedasticidad\n",
    "bp_test = het_breuschpagan(residuos, modelo.model.exog)\n",
    "print(\"Breusch-Pagan p-value:\", bp_test[1])  # p>0.05 → homocedasticidad\n",
    "\n",
    "# --- 3.2 Normalidad de los residuos ---\n",
    "sns.histplot(residuos, kde=True)\n",
    "plt.title(\"Distribución de residuos\")\n",
    "plt.show()\n",
    "\n",
    "sm.qqplot(residuos, line='s')\n",
    "plt.title(\"QQ-plot de residuos\")\n",
    "plt.show()\n",
    "\n",
    "# Test de Anderson-Darling\n",
    "ad_stat, ad_p = normal_ad(residuos)\n",
    "print(\"Normalidad (Anderson-Darling) p-value:\", ad_p)  # p>0.05 → normalidad\n",
    "\n",
    "# --- 3.3 Independencia de errores ---\n",
    "# Test de Durbin-Watson (ya aparece en summary)\n",
    "print(\"Durbin-Watson:\", sm.stats.durbin_watson(residuos))\n",
    "\n",
    "# --- 3.4 Observaciones influyentes ---\n",
    "influence = modelo.get_influence()\n",
    "cooks = influence.cooks_distance[0]\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.stem(np.arange(len(cooks)), cooks, markerfmt=\",\")\n",
    "plt.axhline(4/len(cooks), color=\"red\", linestyle=\"--\", label=\"Umbral 4/n\")\n",
    "plt.xlabel(\"Índice de observación\")\n",
    "plt.ylabel(\"Distancia de Cook\")\n",
    "plt.title(\"Distancias de Cook\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ceb053",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df)\n",
    "sns.color_palette(\"pastel\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03c1a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Variables explicativas y target\n",
    "X = df_clean[[\"X1\", \"X2\", \"X3\", \"X4\"]]\n",
    "y = df_clean[\"Y\"]\n",
    "\n",
    "# Modelo completo\n",
    "Xc = sm.add_constant(X)\n",
    "full_model = sm.OLS(y, Xc).fit()\n",
    "\n",
    "# Función para Cp de Mallows\n",
    "def mallows_cp(model, full_model, n):\n",
    "    SSE_p = np.sum(model.resid ** 2)\n",
    "    MSE_full = np.mean(full_model.resid ** 2)\n",
    "    p = model.df_model + 1  # número de parámetros (incluye constante)\n",
    "    Cp = SSE_p / MSE_full - (n - 2*p)\n",
    "    return Cp\n",
    "\n",
    "# Iterar sobre TODAS las combinaciones posibles\n",
    "resultados = []\n",
    "\n",
    "n = len(df)\n",
    "variables = X.columns\n",
    "\n",
    "for k in range(1, len(variables)+1):\n",
    "    for subset in itertools.combinations(variables, k):\n",
    "        Xs = sm.add_constant(X[list(subset)])\n",
    "        modelo = sm.OLS(y, Xs).fit()\n",
    "        Cp = mallows_cp(modelo, full_model, n)\n",
    "        resultados.append({\n",
    "            \"Variables\": subset,\n",
    "            \"Cp\": Cp,\n",
    "            \"R2\": modelo.rsquared,\n",
    "            \"AIC\": modelo.aic,\n",
    "            \"BIC\": modelo.bic\n",
    "        })\n",
    "\n",
    "# Convertir a DataFrame ordenado\n",
    "df_resultados = pd.DataFrame(resultados)\n",
    "df_resultados = df_resultados.sort_values(by=\"Cp\")\n",
    "df_resultados.reset_index(drop=True, inplace=True)\n",
    "# parece que el modelo con X1, X2 Y X3 es el que mejor se comporta\n",
    "df_resultados\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b928fafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Una vez elegido la combinación de variables\n",
    "def ajustar_modelo(datos, y_var, x_vars):\n",
    "    X = sm.add_constant(datos[x_vars])\n",
    "    y = datos[y_var]\n",
    "    modelo_final = sm.OLS(y, X).fit()\n",
    "    return modelo_final\n",
    "x_finales = [\"X1\", \"X3\", \"X4\"]\n",
    "modelo_final = ajustar_modelo(df_clean, \"Y\", x_finales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe58f5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validamos de nuevo supuestos y evaluamos el nuevo modelo\n",
    "def validar_supuestos(modelo):\n",
    "    residuales = modelo.resid\n",
    "    fitted = modelo.fittedvalues\n",
    "\n",
    "    print(\"\\n--- Normalidad ---\")\n",
    "    shapiro_stat, shapiro_p = stats.shapiro(residuales)\n",
    "    print(f\"Shapiro-Wilk p={shapiro_p:.4f} → {'OK' if shapiro_p > 0.05 else 'X'}\")\n",
    "\n",
    "    print(\"\\n--- Homocedasticidad ---\")\n",
    "    lm, lm_pvalue, fvalue, f_pvalue = het_breuschpagan(residuales, modelo.model.exog)\n",
    "    print(f\"Breusch-Pagan p={lm_pvalue:.4f} → {'OK' if lm_pvalue > 0.05 else 'X'}\")\n",
    "\n",
    "    print(\"\\n--- Independencia ---\")\n",
    "    dw = sm.stats.stattools.durbin_watson(residuales)\n",
    "    print(f\"Durbin-Watson={dw:.2f} (cerca de 2 es ideal)\")\n",
    "\n",
    "    print(\"\\n--- Multicolinealidad ---\")\n",
    "    X = modelo.model.exog\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"Variable\"] = modelo.model.exog_names\n",
    "    vif[\"VIF\"] = [variance_inflation_factor(X, i) for i in range(X.shape[1])]\n",
    "    print(vif)\n",
    "\n",
    "    # Gráficos de diagnóstico\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    sns.residplot(x=fitted, y=residuales, lowess=True, ax=axes[0],\n",
    "                  line_kws={'color': 'red', 'lw': 1})\n",
    "    axes[0].set_title(\"Residuos vs Valores Ajustados\")\n",
    "\n",
    "    sm.qqplot(residuales, line='45', ax=axes[1])\n",
    "    axes[1].set_title(\"Gráfico Q-Q\")\n",
    "    plt.show()\n",
    "validar_supuestos(modelo_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5ed7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Diagnostico de influencia del nuevo modelo\n",
    "def diagnostico_influencia(modelo):\n",
    "    infl = modelo.get_influence()\n",
    "    summary = pd.DataFrame({\n",
    "        \"resid\": modelo.resid,\n",
    "        \"resid_student\": infl.resid_studentized_internal,\n",
    "        \"cooks_d\": infl.cooks_distance[0],\n",
    "        \"hii\": infl.hat_matrix_diag,\n",
    "        \"dffits\": infl.dffits[0]\n",
    "    })\n",
    "    umbral_cook = 4 / len(summary)\n",
    "    print(f\"\\nUmbral de Cook ≈ {umbral_cook:.4f}\")\n",
    "    print(\"Puntos influyentes:\\n\", summary[summary[\"cooks_d\"] > umbral_cook])\n",
    "    return summary\n",
    "diagnostico_influencia(modelo_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c9f700",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_final.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43895f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#f) Realice una predicción utilizando el modelo seleccionado, interprete.\n",
    "\n",
    "# Definimos nuevos datos para predicción\n",
    "nuevos_datos = pd.DataFrame({\n",
    "    'X1': [100, 108, 90, 110, 95, 82, 102],\n",
    "    'X3': [105, 95, 110, 100, 90, 85,98],\n",
    "    'X4': [100, 90, 95, 85, 80, 75, 70]\n",
    "})\n",
    "nuevos_datos = sm.add_constant(nuevos_datos)\n",
    "\n",
    "predictions = modelo_final.get_prediction(nuevos_datos)\n",
    "predictions_summary = predictions.summary_frame(alpha=0.05)  # Intervalo de confianza del 95%\n",
    "\n",
    "# Mostrar las predicciones\n",
    "print(f\"Los valores predichos con los nuevos valores son: \\n {predictions_summary}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
